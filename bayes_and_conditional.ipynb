{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why we need Bayes' theorem and other fun with conditional probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we need to determine the probability that a test result is giving you correct information. For example: suppose you are getting tested for a disease that occurs in 1% of a population. You know that the test has a 2% false positive rate and a 5% false negative rate. If your result comes back positive, what's the probability that you actually have the disease? If it comes back negative, what's the chance that you actully do have the disease?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a table with everything we know about this test on a hypothetical population of 10000. Since it occurs in 1% of the population, 100 people will actually have it while 9900 won't. And of those people who actually have it, if they were all tested, 5 would get a (false) negative result. Of the 9900 people who don't have it, 2% (or 198) would get a (false) positive result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's all that in a table. \n",
    "\n",
    "|               | diseased | not diseased | total |   |\n",
    "|---------------|----------|--------------|-------|---|\n",
    "| test positive | 95       | 198          | 293   |   |\n",
    "| test negative | 5        | 9702         | 9707  |   |\n",
    "| total         | 100      | 9900         | 10000 |   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on to answering the original questions, let's stop here for a moment and define some terms about tests. \n",
    "\n",
    "**Specificity** is the test's ability to correctly identify healthy patients.\n",
    "\n",
    "**Sensitivity** is the test's ability to correctly identify diseased patients.\n",
    "\n",
    "Let's name each cell in the table so we can use them in some equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|               | diseased                | not diseased              | total |   |\n",
    "|---------------|-------------------------|---------------------------|-------|---|\n",
    "| test positive | 95 (true positive / TP) | 198 (false positive / FP) | 293   |   |\n",
    "| test negative | 5 (false negative / FN) | 9702 (true negative / TN) | 9707  |   |\n",
    "| total         | 100 (positive / P)      | 9900 (negative / N)       | 10000 |   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Wikipedia article:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{sensitivity} = \\text{recall} = \\text{true positive rate} = \\text{TPR} =\\frac{TP}{P}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plugging in our values, we have\n",
    "\n",
    "$\\text{TPR} =\\frac{TP}{P} =\\frac{95}{100} = 95\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for specificity:\n",
    "\n",
    "$\\text{specificity} = \\text{true negative rate} = \\text{TNR} =\\frac{TN}{N} =\\frac{9702}{9900} = 98\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another few terms that are worth mentioning are **Accuracy**, the fraction of test results that are correct, and **Precision**, the fraction of true positives to total positives. Here's an example from the [Wikipedia page](https://en.wikipedia.org/wiki/Precision_and_recall) for precision and recall:\n",
    "\n",
    "> Suppose a computer program for recognizing dogs in photographs identifies 8 dogs in a picture containing 12 dogs and some cats. Of the 8 identified as dogs, 5 actually are dogs (true positives), while the rest are cats (false positives). The program's precision is 5/8 while its recall is 5/12. When a search engine returns 30 pages, only 20 of which were relevant, while failing to return 40 additional relevant pages, its precision is 20/30 = 2/3 while its recall is 20/60 = 1/3. So, in this case, precision is \"how valid the search results are\", and recall is \"how complete the results are\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So back to our test from above, here are the definitions and results.\n",
    "\n",
    "$\\text{accuracy} = \\text{ACC} =\\frac{TP + TN}{P + N} =\\frac{95 + 9702}{100 + 9900} = 97.97\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{precision} = \\text{positive predictive value} = \\text{PPV} =\\frac{TP}{TP + FP} =\\frac{95}{95 + 9702} = 0.97\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes' Theorem\n",
    "\n",
    "Okay, but we still haven't answered our original question: if you test positive, what are the chances you actually have the disease? If we think about it and look at the table a bit, we might be tempted to say the following: we tested positive, so we're either in the True Positive or False Positive categories. So, what's the likelihood in being in either category? For True Positive, it would be the fraction of true positives to total positives, or $\\frac{\\text{TP}}{\\text{TP+FP}}$, right?\n",
    "\n",
    "Well, no. Why not? **Let's come back to this.** This is conditional probability, and requires a bit more work. We're going to need to call in Bayes. Bayes' theorem is for this: we want to figure out the probability for some event A, given that B has happened. Specifically, the event B changes the probability of A happening. So if B is rolling a 6 on a 6-sided die while A is rolling a 1, we don't need Bayes' theorem, because B happening tells us nothing about whether or not A is going to happen. But if we're talking about B being the result of a disease test coming back positive or negative, then we definitely do need the theorem. Here's what it says:\n",
    "\n",
    "$\\text{probability of A given B} = \\text{P(A|B)} = \\frac{P(B|A)P(A)}{P(B)}$\n",
    "\n",
    "So let's try this out on our question.\n",
    "\n",
    "$\\text{probability we have disease if positive test result} = \\text{P(disease | positive)} = \\frac{\\text{P(positive | disease)}\\text{P(disease)}}{\\text{P(positive)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{P(positive | disease)}$ is the probability we will test positive if we have the disease, which is the True Positive (TP) rate. And we know the disease rate is 1% and the positive test rate is 1%. So then the probability we have the disease if we test positive is:\n",
    "\n",
    "$\\text{P(disease | positive)} = \\frac{\\text{P(positive | disease)}\\text{P(disease)}}{\\text{P(positive)}} = \\frac{95\\% * 1\\%}{1\\%} = \\frac{.95\\%}{1\\%} = 9.5\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually pretty unlikely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "1. What if the disease rate ticked up to 2% of the population next year? Which measurements would stay the same and which would change?\n",
    "2. What if we developed a better test that halved the false positive rate? How would that affect the chances that you have the disease if you test positive?\n",
    "3. Suppose we have a test with a sensitivity of 50% and specificity of 75%, with a population prevalence of 1%. Make a table like the one above with all the cells filled in."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
